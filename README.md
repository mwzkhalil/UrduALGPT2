# UrduALGPT2
This repository implements an approach that combines ALBERT (A Lite BERT) with the GPT (Generative Pre-trained Transformer) architecture on Urdu Dataset. The project consists of two main files: `train.py` and `model.py`.


## Prerequisites

Before running the code, ensure you have the following prerequisites installed:

- Python 3.7+
- PyTorch
- Transformers library by Hugging Face
- Datasets library by Hugging Face


